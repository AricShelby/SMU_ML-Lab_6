{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS7324 Lab 6 \n",
    "Convolutional Network Architecture\n",
    "\n",
    "Nancy Wight\n",
    "Aric Shelby\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Access: https://www.datacamp.com/tutorial/convolutional-neural-networks-python\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0dcec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code imports the fashion_mnist dataset from the keras.datasets module.\n",
    "• The load_data() function is then called on the fashion_mnist dataset, which returns two tuples: \n",
    "  (train_X, train_Y) and (test_X, test_Y).\n",
    "• These tuples contain the training and testing data, respectively, for the fashion_mnist dataset.\n",
    "• The train_X and test_X tuples contain the input images, while train_Y and test_Y contain the \n",
    "  corresponding output labels for each image.\n",
    "• Overall, this code loads the fashion_mnist dataset and splits it into training and testing sets \n",
    "  for use in a machine learning model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c070327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "(train_X,train_Y), (test_X,test_Y) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ". This code is not actually a code snippet, but rather a comment or statement that specifies the \n",
    "  backend being used by TensorFlow.\n",
    "• The \"backend\" refers to the implementation of the low-level operations used by TensorFlow, \n",
    "  such as matrix multiplication and convolution.\n",
    "• The statement \"Using TensorFlow backend\" simply indicates that the TensorFlow library will be using \n",
    "  its default backend implementation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e955843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Using TensorFlow backend.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code imports the necessary libraries for the program to run, including numpy, keras.utils, \n",
    "  and matplotlib.pyplot.\n",
    "• It then prints the shape of the training and testing data using the train_X.shape, train_Y.shape, \n",
    "  test_X.shape, and test_Y.shape functions.\n",
    "• The %matplotlib inline command is used to display the plots in the Jupyter notebook.\n",
    "• The to_categorical function from Keras is not used in this code snippet.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51915eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code imports the necessary libraries for the program to run, including numpy, keras.utils, \n",
    "  and matplotlib.pyplot.\n",
    "• It then prints the shape of the training and testing data using the train_X.shape, train_Y.shape, \n",
    "  test_X.shape, and test_Y.shape functions.\n",
    "• The %matplotlib inline command is used to display the plots in the Jupyter notebook.\n",
    "• The to_categorical function from Keras is not used in this code snippet.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27584a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This code is printing out the shape of the training and testing data.\n",
    "• The first tuple shows that the training data has a shape of (60000, 28, 28) \n",
    "  which means there are 60,000 images in the training set, each with a size of 28x28 pixels.\n",
    "• The second tuple shows that the testing data has a shape of (10000, 28, 28) which means \n",
    "  there are 10,000 images in the testing set, each with a size of 28x28 pixels.\n",
    "• The comma after each tuple indicates that there is another value in the tuple, \n",
    "  which is the number of labels for each set (60000 labels for the training set and \n",
    "  10000 labels for the testing set).\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc04b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "('Training data shape : ', (60000, 28, 28), (60000,))\n",
    "('Testing data shape : ', (10000, 28, 28), (10000,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ca2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The training data has a shape of 60000 x 28 x 28 since there are 60,000 training samples each of 28 x 28 dimension.\n",
    "The test data has a shape of 10000 x 28 x 28 since there are 10,000 testing samples.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ea3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This code uses the NumPy library to find the unique numbers from the train labels.\n",
    "• First, the np.unique() function is called on the train_Y array to find all the unique values in it.\n",
    "• These unique values represent the different classes in the dataset.\n",
    "• The number of unique classes is then calculated using the len() function and stored in the nClasses variable.\n",
    "• Finally, the total number of outputs and the output classes are printed using the print() function.\n",
    "• Overall, this code is useful for understanding the number and types of classes in a dataset.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the unique numbers from the train labels\n",
    "\n",
    "classes = np.unique(train_Y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853176f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is written in Python.\n",
    "• The first line is a print statement that displays the string \"Total number of outputs : \" \n",
    "  followed by the integer value 10.\n",
    "• The second line is another print statement that displays the string \"Output classes : \" \n",
    "  followed by an array of integers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9].\n",
    "• The data type of the array is specified as uint8, which stands for unsigned 8-bit integer.\n",
    "• Overall, this code snippet is simply displaying some information about the outputs and output classes \n",
    "  in a machine learning model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "('Total number of outputs : ', 10)\n",
    "('Output classes : ', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42099ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "A total of ten output classes that range from 0 to 9.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This code creates a figure with two subplots, each displaying an image and its corresponding label.\n",
    "• The plt.figure(figsize=[5,5]) line sets the size of the figure to be 5 inches by 5 inches.\n",
    "• The first subplot is created with plt.subplot(121), which specifies that the figure should have \n",
    "  1 row, 2 columns, and this subplot should be the first one (i.e. the left one).\n",
    "• The image is displayed using plt.imshow(train_X[0,:,:], cmap='gray'), \n",
    "  which shows the first image in the training data with a grayscale color map.\n",
    "• The title of this subplot is set to the ground truth label of the first training image using plt.title\n",
    "  (\"Ground Truth : {}\".format(train_Y[0])).\n",
    "• The second subplot is created with plt.subplot(122), which specifies that the figure should have \n",
    "  1 row, 2 columns, and this subplot should be the second one (i.e. the right one).\n",
    "• The image is displayed using plt.imshow(test_X[0,:,:], cmap='gray'), \n",
    "  which shows the first image in the testing data with a grayscale color map.\n",
    "• The title of this subplot is set to the ground truth label of the first testing image using \n",
    "  plt.title(\"Ground Truth : {}\".format(test_Y[0])).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(test_Y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b056119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code creates a text object using the Text() function from the matplotlib library.\n",
    "• The text object displays the string \"Ground Truth : 9\" and is centered horizontally at \n",
    "  x-coordinate 0.5 and vertically at y-coordinate 1.\n",
    "• The u before the string indicates that it is a Unicode string.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Text(0.5,1,u'Ground Truth : 9')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "776c1a51",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The images are grayscale images have pixel values that range from 0 to 255. \n",
    "The images have a dimension of 28 x 28. Preprocess the data before the feed into the model.\n",
    "\n",
    "Convert each 28 x 28 image of the train and test set into a matrix of size 28 x 28 x 1 \n",
    "which is fed into the network.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e712f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code reshapes the training and testing data for a machine learning model.\n",
    "• The reshape() function is used to change the shape of an array without changing its data.\n",
    "• In this case, the train_X and test_X arrays are being reshaped to have four dimensions: \n",
    "  the first dimension represents the number of samples, the second and third dimensions represent \n",
    "  the height and width of each image (28x28 pixels), and the fourth dimension represents the \n",
    "  number of color channels (1 for grayscale images).\n",
    "• The -1 argument in the reshape() function is used to automatically calculate the size of the first \n",
    "  dimension based on the size of the other dimensions.\n",
    "• The train_X.shape and test_X.shape statements are used to print the new shapes of the arrays after reshaping.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597377fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_X = train_X.reshape(-1, 28,28, 1)\n",
    "test_X = test_X.reshape(-1, 28,28, 1)\n",
    "train_X.shape, test_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is not actually code, but rather a tuple containing two elements.\n",
    "• The first element is a tuple with dimensions (60000, 28, 28, 1), which likely represents a dataset of \n",
    "  60,000 images with dimensions 28x28 and a single color channel.\n",
    "• The second element is a tuple with dimensions (10000, 28, 28, 1), which likely represents a separate dataset \n",
    "  of 10,000 images with the same dimensions.\n",
    "• This code does not require any specific programming language to understand.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97656f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "((60000, 28, 28, 1), (10000, 28, 28, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c526b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The data is in an int8 format, before feeding it into the network, convert its type to float32, \n",
    "and also rescale the pixel values in range 0 - 1.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is converting the data type of train_X and test_X to float32 using the astype() method.\n",
    "• Then, it is normalizing the pixel values of the images in train_X and test_X by dividing them by 255.\n",
    "• This is done to ensure that the pixel values are in the range of 0 to 1, which is a common preprocessing \n",
    "  step for image data.\n",
    "  \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140af002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Convert the class labels into a one-hot encoding vector.\n",
    "In one-hot encoding, convert the categorical data into a vector of numbers. \n",
    "Convert the categorical data in one hot encoding so that machine learning algorithms cannot work with \n",
    "categorical data directly. Generate one boolean column for each category or class. \n",
    "Only one of these columns could take on the value 1 for each sample. This is the term one-hot encoding.\n",
    "\n",
    "The problem statement, the one hot encoding will be a row vector, and for each image, \n",
    "it will have a dimension of 1 x 10. The important thing to note here is that the vector consists \n",
    "of all zeros except for the class that it represents, and for that, it is 1. \n",
    "For example, the ankle boot image has a label of 9, so for all the ankle boot images, \n",
    "the one hot encoding vector would be [0 0 0 0 0 0 0 0 1 0].\n",
    "\n",
    "Convert the training and testing labels into one-hot encoding vectors.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is used to convert categorical labels into one-hot encoding.\n",
    "• The to_categorical() function from the Keras library is used to perform this conversion.\n",
    "• It takes in an array of categorical labels and returns an array of one-hot encoded labels.\n",
    "• In this code snippet, the train_Y and test_Y arrays are converted to one-hot encoding using \n",
    "  the to_categorical() function and stored in train_Y_one_hot and test_Y_one_hot respectively.\n",
    "• The print() statements are used to display the original label and the corresponding one-hot encoded label \n",
    "  for the first element in the train_Y array.\n",
    "• One-hot encoding is a technique used to represent categorical data as binary vectors.\n",
    "• Each category is represented by a vector of zeros with a single one at the index corresponding to the category.\n",
    "• This technique is commonly used in machine learning algorithms to represent categorical data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ef05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is showing an example of converting a label into a one-hot encoded vector.\n",
    "• The original label is 9, which means it corresponds to the 10th class in a classification problem with \n",
    "  10 classes.\n",
    "• The second line shows the result of converting the label into a one-hot encoded vector.\n",
    "• The vector has 10 elements, with all elements set to 0 except for the 10th element, which is set to 1.\n",
    "• This is because the label corresponds to the 10th class, and in a one-hot encoded vector,    \n",
    "  only the element corresponding to the class is set to 1, while all other elements are set to 0.\n",
    "• The code is using NumPy arrays to represent the one-hot encoded vector.\n",
    "• The array function is used to create the vector, and the elements are specified using a list of values.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "('Original label:', 9)\n",
    "('After conversion to one-hot:', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18097c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Can also print the train_Y_one_hot, which will display a \n",
    "matrix of size 60000 x 10 in which each row depicts one-hot encoding of an image.\n",
    "\n",
    "In machine learning or any data specific task, partition the data correctly. \n",
    "For the model to generalize well, split the training data into two parts, \n",
    "one designed for training and another one for validation. Train the model on 80\\% of the training data \n",
    "and validate it on 20\\% of the remaining training data. This will also help to reduce overfitting \n",
    "since validating the model on the data it would not have seen in training phase, which will help in \n",
    "boosting the test performance.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code imports the train_test_split function from the sklearn.model_selection module.\n",
    "• The train_test_split function is then used to split the train_X and train_Y_one_hot datasets \n",
    "  into training and validation sets.\n",
    "• The test_size parameter is set to 0.2, which means that 20% of the data will be used for validation.\n",
    "• The random_state parameter is set to 13, which ensures that the same random split is generated \n",
    "  each time the code is run.\n",
    "• The resulting datasets are assigned to the variables train_X, valid_X, train_label, and valid_label.\n",
    "• Train_X and train_label are the training data and labels, while valid_X and valid_label are the validation data \n",
    "  and labels.\n",
    "• The code is used to split the dataset into training and validation sets, which is a common practice \n",
    "  in machine learning to evaluate the performance of a model.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, \n",
    "    random_state=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ee61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Check the shape of training and validation set.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is written in Python.\n",
    "• The code is displaying the shape of four different arrays: train_X, valid_X, train_label, and valid_label.\n",
    "• The .shape attribute returns the dimensions of the array as a tuple.\n",
    "• The output of this code will be a tuple containing the shape of train_X, valid_X, train_label, \n",
    "  and valid_label arrays respectively.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e98b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_X.shape,valid_X.shape,train_label.shape,valid_label.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is a tuple containing four elements.\n",
    "• The first element is a NumPy array with a shape of (48000, 28, 28, 1), which represents a dataset of \n",
    "  48,000 images with a height and width of 28 pixels and a single color channel.\n",
    "• The second element is another NumPy array with a shape of (12000, 28, 28, 1), representing a \n",
    "  validation dataset of 12,000 images with the same dimensions as the first dataset.\n",
    "• The third element is a NumPy array with a shape of (48000, 10), representing the labels \n",
    "  for the training dataset, where each label is a one-hot encoded vector of length 10.\n",
    "• The fourth element is another NumPy array with a shape of (12000, 10), representing the labels for the \n",
    "  validation dataset.\n",
    "• Overall, this code is likely part of a machine learning tutorial or project, where the datasets and labels \n",
    "  are being prepared for use in a neural network or other machine learning model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d051d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "((48000, 28, 28, 1), (12000, 28, 28, 1), (48000, 10), (12000, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268de8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The images are of size 28 x 28. You convert the image matrix to an array, rescale it between 0 and 1, \n",
    "reshape it to size 28 x 28 x 1, and feed this as an input to the network.\n",
    "\n",
    "Use three convolutional layers:\n",
    "\n",
    "The first layer will have 32-3 x 3 filters,\n",
    "The second layer will have 64-3 x 3 filters and\n",
    "The third layer will have 128-3 x 3 filters.\n",
    "There are three max-pooling layers each of size 2 x 2.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code imports the necessary modules from the Keras library to build a convolutional neural network (CNN) \n",
    "  model.\n",
    "• Keras is the main module for the Keras library.\n",
    "• Sequential is a class that allows us to create a linear stack of layers of the model.\n",
    "• Input is a class that allows us to define the shape of the input data to the model.\n",
    "• Model is a class that allows us to define a more complex model architecture than the simple linear stack of \n",
    "  layers provided by Sequential.\n",
    "• Dense is a class that represents a fully connected layer in the model.\n",
    "• Dropout is a class that applies dropout regularization to the model, which helps prevent overfitting.\n",
    "• Flatten is a class that flattens the output of a previous layer into a 1D array, which can then be passed \n",
    "  to a fully connected layer.\n",
    "• Conv2D is a class that represents a 2D convolutional layer in the model.\n",
    "• MaxPooling2D is a class that applies max pooling to the output of a previous layer, \n",
    "  which helps reduce the spatial dimensions of the data.\n",
    "• BatchNormalization is a class that applies batch normalization to the output of a previous layer, \n",
    "  which helps improve the stability and speed of training.\n",
    "• LeakyReLU is a class that applies the leaky rectified linear activation function to the output of a \n",
    "  previous layer, which helps prevent the \"dying ReLU\" problem and can improve the performance of the model.\n",
    "• This code sets up the necessary tools to build a CNN model using Keras.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c93bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Use a batch size of 64 using a higher batch size of 128 or 256 is also preferable \n",
    "it all depends on the memory. It contributes massively to determining the learning parameters and \n",
    "affects the prediction accuracy. You will train the network for 20 epochs.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c812df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet defines three variables: batch_size, epochs, and num_classes.\n",
    "• Batch_size is typically used in machine learning to specify the number of samples that will be propagated \n",
    "  through the neural network at once during training.\n",
    "• In this case, the batch size is set to 64.\n",
    "• Epochs refers to the number of times the entire dataset will be passed through the neural network during training.\n",
    "• In this case, the number of epochs is set to 20.\n",
    "• num_classes is the number of classes in the dataset.\n",
    "• This variable is often used in classification tasks to specify the number of possible output classes.\n",
    "• In this case, there are 10 classes.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8152ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "In Keras, stack up layers by adding the desired layer one by one. First add a first convolutional layer \n",
    "with Conv2D(). Use this function, as this is working with images. Add the Leaky ReLU activation function \n",
    "which helps the network learn non-linear decision boundaries. There are ten different classes, use a non-linear \n",
    "decision boundary that could separate these ten classes which are not linearly separable.\n",
    "\n",
    "Add Leaky ReLUs because they attempt to fix the problem of dying Rectified Linear Units (ReLUs). \n",
    "The ReLU activation function is used a lot in neural network architectures and \n",
    "more specifically in convolutional networks, where it has proven to be more effective than \n",
    "the widely used logistic sigmoid function. As of 2017, this activation function is the most popular \n",
    "one for deep neural networks. The ReLU function allows the activation to be thresholded at zero. \n",
    "However, during the training, ReLU units can \"die\". This can happen when a large gradient flows through a \n",
    "ReLU neuron: it can cause the weights to update in such a way that the neuron will never activate on any data \n",
    "point again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. \n",
    "Leaky ReLUs attempt to solve this: the function will not be zero but will instead have a small negative slope.\n",
    "\n",
    "Add the max-pooling layer with MaxPooling2D() and so on. The last layer is a Dense layer that has a softmax \n",
    "activation function with 10 units, which is needed for this multi-class classification problem.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1179d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code creates a convolutional neural network (CNN) model for image classification using the Keras library \n",
    "  in Python.\n",
    "• The Sequential() function initializes an empty model object.\n",
    "• The Conv2D() function adds a 2D convolutional layer to the model with 32 filters, a kernel size of 3x3, \n",
    "  and a linear activation function.\n",
    "• The input_shape parameter specifies the shape of the input images (28x28 pixels with 1 color channel).\n",
    "• The padding parameter is set to 'same', which means that the output feature maps will have the same \n",
    "  spatial dimensions as the input feature maps.\n",
    "• The LeakyReLU() function adds a leaky rectified linear unit activation function to the model with an \n",
    "  alpha value of 0.1.\n",
    "• This helps to prevent the vanishing gradient problem and improve the model's performance.\n",
    "• The MaxPooling2D() function adds a 2D max pooling layer to the model with a pool size of 2x2 and 'same' padding.\n",
    "• This reduces the spatial dimensions of the feature maps and helps to prevent overfitting.\n",
    "• The above steps are repeated with increasing number of filters (64 and 128) and the same kernel size and padding.\n",
    "• The Flatten() function flattens the output of the previous layer into a 1D array, which can be fed into a \n",
    "  fully connected layer.\n",
    "• The Dense() function adds a fully connected layer to the model with 128 neurons and a linear activation function.\n",
    "• The LeakyReLU() function is again added to the model with an alpha value of 0.1.\n",
    "• Finally, the Dense() function adds an output layer to the model with a number of neurons equal to the \n",
    "  number of classes in the dataset and a softmax activation function.\n",
    "• This allows the model to output a probability distribution over the classes for each input image.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "After the model is created, compile it using the Adam optimizer, one of the most popular optimization algorithms. \n",
    "Specify the loss type which is categorical cross entropy which is used for multi-class classification, \n",
    "Use the binary cross-entropy as the loss function. Specify the metrics as accuracy \n",
    "to analyze while the model is training.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code compiles a deep learning model for fashion classification using the Keras library in Python.\n",
    "• The compile() function is used to configure the model for training.\n",
    "• It takes three arguments: loss, optimizer, and metrics.\n",
    "• Loss is the objective function that the model will try to minimize during training.\n",
    "• In this case, keras.losses.categorical_crossentropy is used, which is a common loss function \n",
    "  for multi-class classification problems.\n",
    "• Optimizer is the algorithm used to update the weights of the model during training.\n",
    "• Here, keras.optimizers.Adam() is used, which is a popular optimization algorithm that adapts the \n",
    "  learning rate during training.\n",
    "• Metrics is a list of metrics used to evaluate the performance of the model during training and testing.\n",
    "• In this case, the metric used is accuracy, which is a common metric for classification problems that \n",
    "  measures the percentage of correctly classified samples.\n",
    "• This code compiles a deep learning model for fashion classification using the categorical cross-entropy \n",
    "  loss function, the Adam optimization algorithm, and the accuracy metric.\n",
    "  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb276f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf76fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Visualize the layers step by using the summary function. \n",
    "This will show some parameters (weights and biases) in each layer and also the total parameters in the model.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d22ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is written in Python.\n",
    "• The fashion_model object is a machine learning model that has been previously defined and trained.\n",
    "• The summary() method is called on this object to display a summary of the model's architecture, \n",
    "  including the number of layers, the shape of the input and output tensors, and the number of trainable \n",
    "  parameters in each layer.\n",
    "• This information can be useful for understanding the structure and complexity of the model.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fashion_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code defines a neural network architecture using the Keras library.\n",
    "• The architecture consists of several layers, each with a specific type and output shape.\n",
    "• The first layer is a convolutional layer (Conv2D) with 32 filters and a kernel size of 3x3.\n",
    "• This layer takes in an input image of size 28x28 and outputs a feature map of size 28x28x32.\n",
    "• The second layer is a LeakyReLU activation layer (LeakyReLU) that applies a non-linear activation \n",
    "  function to the output of the previous layer.\n",
    "• The third layer is a max pooling layer (MaxPooling) that reduces the spatial dimensions of the feature \n",
    "  map by taking the maximum value within a 2x2 window.\n",
    "• This layer outputs a feature map of size 14x14x32.\n",
    "• The fourth layer is another convolutional layer with 64 filters and a kernel size of 3x3.\n",
    "• This layer takes in the output of the previous layer and outputs a feature map of size 14x14x64.\n",
    "• The fifth layer is another LeakyReLU activation layer that applies a non-linear activation function \n",
    "  to the output of the previous layer.\n",
    "• The sixth layer is another max pooling layer that reduces the spatial dimensions of the feature map by \n",
    "  taking the maximum value within a 2x2 window.\n",
    "• This layer outputs a feature map of size 7x7x64.\n",
    "• The seventh layer is another convolutional layer with 128 filters and a kernel size of 3x3.\n",
    "• This layer takes in the output of the previous layer and outputs a feature map of size 7x7x128.\n",
    "• The eighth layer is another LeakyReLU activation layer that applies a non-linear activation function \n",
    "  to the output of the previous layer.\n",
    "• The ninth layer is another max pooling layer that reduces the spatial dimensions of the feature map by \n",
    "  taking the maximum value within a 2x2 window.\n",
    "• This layer outputs a feature map of size 4x4x128.\n",
    "• The tenth layer is a flatten layer (Flatten) that converts the 4x4x128 feature map into a 1D vector of \n",
    "  length 2048.\n",
    "• The eleventh layer is a fully connected layer (Dense) with 128 neurons that takes in the 2048-dimensional \n",
    "  vector and outputs a 128-dimensional vector.\n",
    "• The twelfth layer is another LeakyReLU activation layer that applies a non-linear activation function \n",
    "  to the output of the previous layer.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_51 (Conv2D)           (None, 28, 28, 32)        320       \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_57 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_49 (MaxPooling (None, 14, 14, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_52 (Conv2D)           (None, 14, 14, 64)        18496     \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_58 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_50 (MaxPooling (None, 7, 7, 64)          0         \n",
    "_________________________________________________________________\n",
    "conv2d_53 (Conv2D)           (None, 7, 7, 128)         73856     \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_59 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_51 (MaxPooling (None, 4, 4, 128)         0         \n",
    "_________________________________________________________________\n",
    "flatten_17 (Flatten)         (None, 2048)              0         \n",
    "_________________________________________________________________\n",
    "dense_33 (Dense)             (None, 128)               262272    \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_60 (LeakyReLU)   (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_34 (Dense)             (None, 10)                1290      \n",
    "=================================================================\n",
    "Total params: 356,234\n",
    "Trainable params: 356,234\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fba0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Train the model with Keras' fit() function! The model trains for 20 epochs. \n",
    "The fit() function will return a history object; By storying the result of this function in fashion_train, \n",
    "use it later to plot the accuracy and loss function plots between training and validation which will help \n",
    "to analyze your model's performance visually.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc11ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is using the fit() method of a fashion_model object to train the model on the train_X and train_label \n",
    "  data.\n",
    "• The batch_size parameter specifies the number of samples per gradient update.\n",
    "• The epochs parameter specifies the number of times the training data is passed through the model.\n",
    "• The verbose parameter controls the amount of information printed during training.\n",
    "• A value of 1 means progress bars will be displayed during training.\n",
    "• The validation_data parameter specifies the data on which to evaluate the loss and any model metrics at the end \n",
    "  of each epoch.\n",
    "• In this case, it is using the valid_X and valid_label data.\n",
    "• The fit() method returns a History object that contains information about the training process, \n",
    "  such as the loss and accuracy at each epoch.\n",
    "• The code is assigning this object to the fashion_train variable.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fashion_train = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "    validation_data=(valid_X, valid_label\n",
    "                     \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479159b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet shows the training and validation accuracy and loss values for a neural network model \n",
    "  over 20 epochs.\n",
    "• The model is trained on a dataset of 48,000 samples and validated on a dataset of 12,000 samples.\n",
    "• During each epoch, the model is trained on the training dataset and the validation accuracy and loss \n",
    "  values are calculated on the validation dataset.\n",
    "• The acc and val_acc values represent the accuracy of the model on the training and validation datasets, \n",
    "  respectively.\n",
    "• The loss and val_loss values represent the loss of the model on the training and validation datasets, \n",
    "  respectively.\n",
    "• The fit method of the Keras API is used to train the model.\n",
    "• The fit method takes in the training and validation datasets, the number of epochs to train for, \n",
    "  and other parameters such as batch size and optimizer.\n",
    "• Overall, this code snippet shows the training and validation performance of a neural network model over \n",
    "  multiple epochs.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca594b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Train on 48000 samples, validate on 12000 samples\n",
    "Epoch 1/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.4661 - acc: 0.8311 - val_loss: 0.3320 - \n",
    "            val_acc: 0.8809\n",
    "Epoch 2/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.2874 - acc: 0.8951 - val_loss: 0.2781 - \n",
    "            val_acc: 0.8963\n",
    "Epoch 3/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.2420 - acc: 0.9111 - val_loss: 0.2501 - \n",
    "            val_acc: 0.9077\n",
    "Epoch 4/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.2088 - acc: 0.9226 - val_loss: 0.2369 - \n",
    "            val_acc: 0.9147\n",
    "Epoch 5/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.1838 - acc: 0.9324 - val_loss: 0.2602 - \n",
    "            val_acc: 0.9070\n",
    "Epoch 6/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.1605 - acc: 0.9396 - val_loss: 0.2264 - \n",
    "            val_acc: 0.9193\n",
    "Epoch 7/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.1356 - acc: 0.9488 - val_loss: 0.2566 - \n",
    "            val_acc: 0.9180\n",
    "Epoch 8/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.1186 - acc: 0.9553 - val_loss: 0.2556 - \n",
    "            val_acc: 0.9149\n",
    "Epoch 9/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.0985 - acc: 0.9634 - val_loss: 0.2681 - \n",
    "            val_acc: 0.9204\n",
    "Epoch 10/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.0873 - acc: 0.9670 - val_loss: 0.2712 - \n",
    "            val_acc: 0.9221\n",
    "Epoch 11/20\n",
    "48000/48000 [==============================] - 59s 1ms/step - loss: 0.0739 - acc: 0.9721 - val_loss: 0.2757 - \n",
    "            val_acc: 0.9202\n",
    "Epoch 12/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0628 - acc: 0.9767 - val_loss: 0.3126 - \n",
    "            val_acc: 0.9132\n",
    "Epoch 13/20\n",
    "48000/48000 [==============================] - 61s 1ms/step - loss: 0.0569 - acc: 0.9789 - val_loss: 0.3556 - \n",
    "            val_acc: 0.9081\n",
    "Epoch 14/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0452 - acc: 0.9833 - val_loss: 0.3441 - \n",
    "            val_acc: 0.9189\n",
    "Epoch 15/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0421 - acc: 0.9847 - val_loss: 0.3400 - \n",
    "            val_acc: 0.9165\n",
    "Epoch 16/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0379 - acc: 0.9861 - val_loss: 0.3876 - \n",
    "            val_acc: 0.9195\n",
    "Epoch 17/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0405 - acc: 0.9855 - val_loss: 0.4112 - \n",
    "            val_acc: 0.9164\n",
    "Epoch 18/20\n",
    "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0285 - acc: 0.9897 - val_loss: 0.4150 - \n",
    "            val_acc: 0.9181\n",
    "Epoch 19/20\n",
    "48000/48000 [==============================] - 61s 1ms/step - loss: 0.0322 - acc: 0.9877 - val_loss: 0.4584 - \n",
    "            val_acc: 0.9196\n",
    "Epoch 20/20\n",
    "48000/48000 [==============================] - 61s 1ms/step - loss: 0.0262 - acc: 0.9906 - val_loss: 0.4396 - \n",
    "            val_acc: 0.9205\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Trained the model on fashion-MNIST for 20 epochs, and by observing the training accuracy and loss, \n",
    "he model did a good job since after 20 epochs the training accuracy is 99% and the training loss is quite low.\n",
    "\n",
    "The model is overfitting, as the validation loss is 0.4396 and the validation accuracy is 92%. \n",
    "Overfitting gives an intuition that the network has memorized the training data very well \n",
    "but is not guaranteed to work on unseen data, and that is why there is a difference in the \n",
    "training and validation accuracy.\n",
    "\n",
    "Make the model perform much better by adding a Dropout layer into the network and keeping \n",
    "all the other layers unchanged.\n",
    "\n",
    "Evaluate the performance of the model on the test set.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7416780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is evaluating the performance of a trained machine learning model called fashion_model \n",
    "  on a test dataset.\n",
    "• The evaluate() method is called on the fashion_model object with three arguments: \n",
    "  test_X, test_Y_one_hot, and verbose.\n",
    "• test_X is the input test data, test_Y_one_hot is the one-hot encoded labels for the test data, and verbose \n",
    "  is an optional argument that controls the verbosity of the evaluation output.\n",
    "• The evaluate() method returns a list of evaluation metrics, such as accuracy, loss, etc.\n",
    "• Since verbose is set to 0, the output is suppressed and not printed to the console.\n",
    "• The evaluation metrics can be accessed and used for further analysis or reporting.\n",
    "• This code is a common step in the machine learning workflow to assess the performance of a trained model \n",
    " on unseen data.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017adb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_eval = fashion_model.evaluate(test_X, test_Y_one_hot, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is using the Python programming language to print out the test loss and test accuracy \n",
    "  of a machine learning model.\n",
    "• The print() function is used to display the text \"Test loss:\" and the value of test_eval[0], \n",
    "  which is assumed to be the test loss.\n",
    "• Similarly, the function is used again to display the text \"Test accuracy:\" and the value of test_eval[1], \n",
    "  which is assumed to be the test accuracy.\n",
    "• Overall, this code is simply outputting the test loss and test accuracy values for a machine learning model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ea3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09038613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The classifier does better than the benchmark that was reported, which is an SVM classifier \n",
    "with mean accuracy of 0.897. \n",
    "\n",
    "The model looked like it was overfitting. \n",
    "\n",
    "Plot the accuracy and loss plots between training and validation data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed494e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is used to plot the training and validation accuracy and loss of a machine learning model.\n",
    "• First, the code extracts the accuracy and loss values from the history attribute of the fashion_train object.\n",
    "• The history attribute is a dictionary that contains the training and validation accuracy and loss values for \n",
    "  each epoch of the model training.\n",
    "• Next, the epochs variable is created as a range object with the length of the accuracy list.\n",
    "• This will be used as the x-axis for the plots.\n",
    "• Then, the plt.plot() function is used to plot the training and validation accuracy and loss values against the \n",
    "  epochs range.\n",
    "• The 'bo' and 'b' arguments specify the color and marker style of the plot.\n",
    "• The 'bo' argument means blue circles, while 'b' means a solid blue line.\n",
    "• The label argument is used to specify the label for each plot.\n",
    "• After plotting the accuracy and loss values, the plt.title() function is used to set the title for each plot, \n",
    "  and the plt.legend() function is used to add a legend to the plot.\n",
    "• The plt.show() function is called to display the plot.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e96384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy = fashion_train.history['acc']\n",
    "val_accuracy = fashion_train.history['val_acc']\n",
    "loss = fashion_train.history['loss']\n",
    "val_loss = fashion_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The validation accuracy almost became stagnant after 4-5 epochs and rarely increased at certain epochs. \n",
    "In the beginning, the validation accuracy was linearly increasing with loss, but then it did not increase much.\n",
    "\n",
    "The validation loss shows that this is the sign of overfitting, similar to validation accuracy it \n",
    "linearly decreased but after 4-5 epochs, it started to increase. This means that the model tried to \n",
    "memorize the data and succeeded.\n",
    "\n",
    "Use the dropout into our model and see if it helps in reducing overfitting.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e463b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Add a dropout layer to overcome the problem of overfitting to some extent. Dropout randomly turns off a \n",
    "fraction of neurons during the training process, reducing the dependency on the training set by some amount. \n",
    "How many fractions of neurons you want to turn off is decided by a hyperparameter, which can be tuned accordingly. \n",
    "This way, turning off some neurons will not allow the network to memorize the training data since not all the \n",
    "neurons will be active at the same time and the inactive neurons will not be able to learn anything.\n",
    "\n",
    "Create, compile and train the network again but this time with dropout. Run it for 20 epochs with a batch size of \n",
    "64.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5988712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet defines three variables: batch_size, epochs, and num_classes.\n",
    "• batch_size is typically used in machine learning to specify the number of samples that will be \n",
    "  propagated through the neural network at once during training.\n",
    "• The batch size is set to 64.\n",
    "• Epochs refers to the number of times the entire dataset will be passed through the neural network during training.\n",
    "• The number of epochs is set to 20.\n",
    "• num_classes is the number of classes in the dataset.\n",
    "• This variable is often used in classification tasks to specify the number of possible output classes.\n",
    "• There are 10 classes.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code creates a convolutional neural network (CNN) model for image classification.\n",
    "• The Sequential() function initializes an empty model.\n",
    "• The Conv2D() function adds a convolutional layer with 32 filters, a kernel size of 3x3, and a linear \n",
    "  activation function.\n",
    "• The padding parameter is set to 'same', which means that the output size of the layer will be the same \n",
    "  as the input size.\n",
    "• The input_shape parameter specifies the shape of the input data, which is a 28x28 grayscale image with \n",
    "  a single channel.\n",
    "• The LeakyReLU() function adds a leaky rectified linear unit activation function with an alpha \n",
    "  value of 0.1 to introduce non-linearity to the model.\n",
    "• The MaxPooling2D() function adds a max pooling layer with a pool size of 2x2 and a padding of \n",
    "  'same' to reduce the spatial dimensions of the output.\n",
    "• The Dropout() function adds a dropout layer with a rate of 0.25 or 0.4 to prevent overfitting by randomly \n",
    "  dropping out some of the neurons during training.\n",
    "• The above steps are repeated with increasing number of filters and decreasing spatial dimensions \n",
    "  until the output is flattened using the Flatten() function.\n",
    "• The Dense() function adds a fully connected layer with 128 neurons and a linear activation function.\n",
    "• The softmax activation function is used in the output layer with the number of classes specified by num_classes.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13092730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(28,28,1)))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.25))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.25))\n",
    "fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Dropout(0.4))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))           \n",
    "fashion_model.add(Dropout(0.3))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is written in Python.\n",
    "• The fashion_model object is a machine learning model that has been previously defined and trained.\n",
    "• The summary() method is called on this object to display a summary of the model's architecture, \n",
    "  including the number of layers, the shape of the input and output tensors, and the number of trainable \n",
    "  parameters in each layer.\n",
    "• This information can be useful for understanding the structure and complexity of the model.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aaee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fashion_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49737f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code defines a convolutional neural network (CNN) architecture for image classification.\n",
    "• The architecture consists of several layers, each with a specific function.\n",
    "• The first layer is a convolutional layer (Conv2D) with 32 filters and a kernel size of 3x3.\n",
    "• This layer takes the input image and applies 32 filters to it, producing 32 feature maps.\n",
    "• The second layer is a leaky rectified linear unit (LeakyReLU) activation function, which \n",
    "  introduces a small negative slope to the standard ReLU function to prevent the \"dying ReLU\" problem.\n",
    "• The third layer is a max pooling layer (MaxPooling) with a pool size of 2x2.\n",
    "• This layer reduces the spatial dimensions of the feature maps by taking the maximum value within \n",
    "  each 2x2 region.\n",
    "• The fourth layer is a dropout layer (Dropout) with a rate of 0.25.\n",
    "• This layer randomly drops out 25% of the neurons to prevent overfitting.\n",
    "• The next three layers repeat the pattern of a convolutional layer, followed by a leaky ReLU \n",
    "  activation function, max pooling layer, and dropout layer.\n",
    "• The number of filters in each convolutional layer increases from 32 to 64 to 128, and the spatial dimensions \n",
    "  of the feature maps decrease accordingly.\n",
    "• The seventh layer is a flatten layer (Flatten) that converts the 4D tensor output of the previous layer \n",
    "  into a 2D tensor.\n",
    "• The eighth layer is a fully connected layer (Dense) with 128 neurons and a leaky ReLU activation function.\n",
    "• The ninth layer is another dropout layer with a rate of 0.5.\n",
    "• The final layer is another fully connected layer with 10 neurons, corresponding to the 10 possible classes \n",
    "  in the MNIST dataset.\n",
    "• This layer uses a softmax activation function to produce a probability distribution over the classes.\n",
    "• The Total params line shows the total number of trainable parameters in the model, which is used to \n",
    "  optimize the weights during training.\n",
    "• The Trainable params line shows the number of parameters that will be updated during training, \n",
    "  while the Non-trainable params line shows the number of parameters that are fixed and not updated during \n",
    "  training.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_54 (Conv2D)           (None, 28, 28, 32)        320       \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_61 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_52 (MaxPooling (None, 14, 14, 32)        0         \n",
    "_________________________________________________________________\n",
    "dropout_29 (Dropout)         (None, 14, 14, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_55 (Conv2D)           (None, 14, 14, 64)        18496     \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_62 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_53 (MaxPooling (None, 7, 7, 64)          0         \n",
    "_________________________________________________________________\n",
    "dropout_30 (Dropout)         (None, 7, 7, 64)          0         \n",
    "_________________________________________________________________\n",
    "conv2d_56 (Conv2D)           (None, 7, 7, 128)         73856     \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_63 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_54 (MaxPooling (None, 4, 4, 128)         0         \n",
    "_________________________________________________________________\n",
    "dropout_31 (Dropout)         (None, 4, 4, 128)         0         \n",
    "_________________________________________________________________\n",
    "flatten_18 (Flatten)         (None, 2048)              0         \n",
    "_________________________________________________________________\n",
    "dense_35 (Dense)             (None, 128)               262272    \n",
    "_________________________________________________________________\n",
    "leaky_re_lu_64 (LeakyReLU)   (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dropout_32 (Dropout)         (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_36 (Dense)             (None, 10)                1290      \n",
    "=================================================================\n",
    "Total params: 356,234\n",
    "Trainable params: 356,234\n",
    "Non-trainable params: 0\n",
    "                  \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbff4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code compiles a deep learning model for fashion classification using the Keras library in Python.\n",
    "• The compile() function is used to configure the model for training.\n",
    "• It takes three arguments: loss, optimizer, and metrics.\n",
    "• loss is the objective function that the model will try to minimize during training.\n",
    "• In this case, keras.losses.categorical_crossentropy is used, which is a common loss function for multi-class \n",
    "  classification problems.\n",
    "• Optimizer is the algorithm used to update the weights of the model during training.\n",
    "• Here, keras.optimizers.Adam() is used, which is a popular optimization algorithm that adapts the learning \n",
    "  rate during training.\n",
    "• metrics is a list of metrics used to evaluate the performance of the model during training and testing.\n",
    "• In this case, the metric used is accuracy, which is a common metric for classification problems that \n",
    "  measures the percentage of correctly classified samples.\n",
    "• This code compiles a deep learning model for fashion classification using the categorical cross-entropy \n",
    "  loss function, the Adam optimization algorithm, and the accuracy metric.\n",
    "  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28033f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is fitting a deep learning model called fashion_model to the training data train_X and train_label.\n",
    "• The batch_size and epochs parameters specify the size of the mini-batches used during training and the number \n",
    "  of times the entire training dataset is passed through the model, respectively.\n",
    "• The verbose parameter controls the amount of output printed during training, with a value of 1 indicating \n",
    "  that progress updates should be printed.\n",
    "• The validation_data parameter specifies the validation dataset to be used during training to monitor the \n",
    "  model's performance on data it has not seen before.\n",
    "• The fit method returns a History object that contains information about the training process, such as the \n",
    "  loss and accuracy at each epoch.\n",
    "• The fashion_train_dropout variable is used to store this History object.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fashion_train_dropout = fashion_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,\n",
    "    verbose=1,validation_data=(valid_X, valid_lab\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c00e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet shows the training and validation accuracy and loss for a neural network model over 20 epochs.\n",
    "• The model was trained on a dataset of 48,000 samples and validated on a dataset of 12,000 samples.\n",
    "• Each epoch represents one complete pass through the training dataset.\n",
    "• During each epoch, the model updates its weights based on the error between its predictions and the actual \n",
    " labels.\n",
    "• The loss metric measures the error between the predicted and actual labels, while the accuracy metric \n",
    "  measures the percentage of correctly classified samples.\n",
    "• The acc and val_acc metrics represent the training and validation accuracy, respectively, while the loss \n",
    "  and val_loss metrics represent the training and validation loss, respectively.\n",
    "• The goal of training a neural network is to minimize the loss and maximize the accuracy on the validation set, \n",
    "  while avoiding overfitting to the training set.\n",
    "• In this code snippet, we can see that the model achieves a training accuracy of 92.55% and a validation \n",
    "  accuracy of 92.69% after 20 epochs.\n",
    "• The training loss decreases over time, indicating that the model is learning to make better predictions.\n",
    "• The validation loss also decreases over time, indicating that the model is not overfitting to the training set.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b566e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Train on 48000 samples, validate on 12000 samples\n",
    "Epoch 1/20\n",
    "48000/48000 [==============================] - 66s 1ms/step - loss: 0.5954 - acc: 0.7789 - val_loss: 0.3788 - \n",
    "            val_acc: 0.8586\n",
    "Epoch 2/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.3797 - acc: 0.8591 - val_loss: 0.3150 - \n",
    "            val_acc: 0.8832\n",
    "Epoch 3/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.3302 - acc: 0.8787 - val_loss: 0.2836 - \n",
    "            val_acc: 0.8961\n",
    "Epoch 4/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.3034 - acc: 0.8868 - val_loss: 0.2663 - \n",
    "            val_acc: 0.9002\n",
    "Epoch 5/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2843 - acc: 0.8936 - val_loss: 0.2481 - \n",
    "            val_acc: 0.9083\n",
    "Epoch 6/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2699 - acc: 0.9002 - val_loss: 0.2469 - \n",
    "            val_acc: 0.9032\n",
    "Epoch 7/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2561 - acc: 0.9049 - val_loss: 0.2422 - \n",
    "            val_acc: 0.9095\n",
    "Epoch 8/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2503 - acc: 0.9068 - val_loss: 0.2429 - \n",
    "            val_acc: 0.9098\n",
    "Epoch 9/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2437 - acc: 0.9096 - val_loss: 0.2230 - \n",
    "            val_acc: 0.9173\n",
    "Epoch 10/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2307 - acc: 0.9126 - val_loss: 0.2170 - \n",
    "            val_acc: 0.9187\n",
    "Epoch 11/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2307 - acc: 0.9135 - val_loss: 0.2265 - \n",
    "            val_acc: 0.9193\n",
    "Epoch 12/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2229 - acc: 0.9160 - val_loss: 0.2136 - \n",
    "            val_acc: 0.9229\n",
    "Epoch 13/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2202 - acc: 0.9162 - val_loss: 0.2173 - \n",
    "            val_acc: 0.9187\n",
    "Epoch 14/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2161 - acc: 0.9188 - val_loss: 0.2142 - \n",
    "            val_acc: 0.9211\n",
    "Epoch 15/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2119 - acc: 0.9196 - val_loss: 0.2133 - \n",
    "            val_acc: 0.9233\n",
    "Epoch 16/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2073 - acc: 0.9222 - val_loss: 0.2159 - \n",
    "            val_acc: 0.9213\n",
    "Epoch 17/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2050 - acc: 0.9231 - val_loss: 0.2123 - \n",
    "            val_acc: 0.9233\n",
    "Epoch 18/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.2016 - acc: 0.9238 - val_loss: 0.2191 - \n",
    "            val_acc: 0.9235\n",
    "Epoch 19/20\n",
    "48000/48000 [==============================] - 65s 1ms/step - loss: 0.2001 - acc: 0.9244 - val_loss: 0.2110 - \n",
    "            val_acc: 0.9258\n",
    "Epoch 20/20\n",
    "48000/48000 [==============================] - 64s 1ms/step - loss: 0.1972 - acc: 0.9255 - val_loss: 0.2092 - \n",
    "            val_acc: 0.9269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca29d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Save the model to directly load it and not have to train it again for 20 epochs. This way, you can load the model\n",
    "later on if needed and modify the architecture. Start the training process on the saved model. \n",
    "It is always a good idea to save the model -and even the model's weights because it saves time. \n",
    "Also save the model after every epoch so that, if some issue occurs that stops the training at an epoch. \n",
    "It will not be necessary to start the training from the beginning.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a45a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code saves the trained model named \"fashion_model\" as a file with the name \"fashion_model_dropout.h5py\".\n",
    "• The \".h5py\" extension indicates that the file is in the Hierarchical Data Format version 5 (HDF5) format, \n",
    "  which is commonly used for storing large numerical datasets.\n",
    "• This saved model can be loaded later for further use or deployment.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db51877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fashion_model.save(\"fashion_model_dropout.h5py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240845b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Evaluate the new model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is evaluating the performance of a trained machine learning model called fashion_model on a \n",
    "  test dataset.\n",
    "• The evaluate() method is called on the fashion_model object with three arguments: test_X, test_Y_one_hot, \n",
    "  and verbose.\n",
    "• test_X is the input test data, test_Y_one_hot is the one-hot encoded labels for the test data, and \n",
    "  verbose is an optional argument that controls the verbosity of the output.\n",
    "• The evaluate() method returns a list of evaluation metrics, such as accuracy, loss, etc.\n",
    "• The test_eval variable is assigned to this list of metrics.\n",
    "• The verbose argument is set to 1, which means that the evaluation progress and results will be displayed \n",
    "  on the console.\n",
    "• If verbose is set to 0, no output will be displayed.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_eval = fashion_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8dd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is not actually a code snippet, but rather a progress bar output that is commonly seen in machine \n",
    "  learning frameworks such as Keras or TensorFlow.\n",
    "• The output shows that the model has processed 10000 samples out of 10000 samples, and the equal sign bars \n",
    "  indicate the progress of the processing.\n",
    "• The \"- 5s 461us/step\" part indicates the time it took to process each step.\n",
    "• Overall, this output is used to track the progress of the model's training or evaluation process.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "10000/10000 [==============================] - 5s 461us/step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is using the Python programming language to print out the test loss and test accuracy \n",
    "  of a machine learning model.\n",
    "• The print() function is used to display the text \"Test loss:\" and the value of test_eval[0], which is assumed \n",
    "  to be the test loss.\n",
    "• Similarly, the function is used again to display the text \"Test accuracy:\" and the value of test_eval[1], \n",
    "  which is assumed to be the test accuracy.\n",
    "• This code is outputting the test loss and test accuracy values for a machine learning model.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is simply printing out the test loss and test accuracy of a machine learning model.\n",
    "• The values are enclosed in parentheses and separated by a comma.\n",
    "• The first value is the test loss, which is a measure of how well the model is able to predict the correct \n",
    "  output for the test data.\n",
    "• The second value is the test accuracy, which is the percentage of test data that the model correctly predicted.\n",
    "• The values are printed out using the print function in Python.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b106e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "('Test loss:', 0.21460009642243386)\n",
    "('Test accuracy:', 0.92300000000000004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0481fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Adding Dropout in the model worked, even though the test accuracy did not improve significantly \n",
    "but the test loss decreased compared to the previous results.\n",
    "\n",
    "Plot the accuracy and loss plots between training and validation data for the one last time.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f86ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is used to plot the training and validation accuracy and loss of a neural network model.\n",
    "• First, the code extracts the accuracy and loss values from the history attribute of the \n",
    "  fashion_train_dropout object.\n",
    "• The history attribute is a dictionary that contains the training and validation metrics of the model at each \n",
    "  epoch.\n",
    "• Next, the range function is used to create a list of integers from 0 to the number of epochs.\n",
    "• This list is used as the x-axis for the plots.\n",
    "• Then, the plt.plot function is used to plot the training and validation accuracy and loss values \n",
    "  against the epochs.\n",
    "• The 'bo' and 'b' arguments specify the color and marker style of the plot.\n",
    "• The label argument is used to create a legend for the plot.\n",
    "• After plotting the accuracy and loss values, the plt.title function is used to add a title to the plot, \n",
    "  and the plt.legend function is used to display the legend.\n",
    "• Finally, the plt.show function is called to display the plot.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de74fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The validation loss and validation accuracy both are in sync with the training loss and training accuracy. \n",
    "Even though the validation loss and accuracy line are not linear, but it shows that your model is not overfitting: \n",
    "the validation loss is decreasing and not increasing, and there is not much gap between training and validation \n",
    "accuracy.\n",
    "\n",
    "The model's generalization capability became much better since the loss on both test set and validation set was \n",
    "only slightly more compared to the training loss.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa31b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = fashion_train_dropout.history['acc']\n",
    "val_accuracy = fashion_train_dropout.history['val_acc']\n",
    "loss = fashion_train_dropout.history['loss']\n",
    "val_loss = fashion_train_dropout.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640efd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    ". The code uses the predict method of the fashion_model object to make predictions on the test_X data.\n",
    "• The test_X data is passed as an argument to the predict method, and the output is assigned to the \n",
    "  predicted_classes variable.\n",
    "• The predict method uses the trained model to make predictions on the input data.\n",
    "• The fashion_model object is a machine learning model that has been trained on fashion image data.\n",
    "• The test_X data is a set of images that the model has not seen before, and the predict method \n",
    "  uses the learned parameters of the model to predict the class labels for each image in the test_X data.\n",
    "• The output of the predict method is an array of predicted class labels, with one label for each input image.\n",
    "• The predicted_classes variable is used to store this output for further analysis or evaluation.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_classes = fashion_model.predict(test_X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcf6d99f",
   "metadata": {},
   "source": [
    "#### \"\"\"\n",
    "\n",
    "The predictions are floating point values, it will not be feasible to compare the predicted labels with \n",
    "true test labels. It is necessary to round off the output which will convert the float values into an integer. \n",
    "Use np.argmax() to select the index number which has a higher value in a row.\n",
    "\n",
    "For example, a prediction for one test image to be 0 1 0 0 0 0 0 0 0 0, \n",
    "the output for this should be a class label 1.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70fa876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is written in Python and it is used to convert the predicted probabilities of a classification \n",
    "  model into predicted classes.\n",
    "• The np.round() function is used to round off the predicted probabilities to the nearest integer.\n",
    "• The np.argmax() function is then used to find the index of the maximum value in each row of the resulting array.\n",
    "• This index corresponds to the predicted class for that particular observation.\n",
    "• The axis=1 parameter is used to specify that the maximum value should be found along the rows of the array.\n",
    "• This is because the predicted probabilities are usually arranged in a matrix where each row corresponds \n",
    "  to an observation and each column corresponds to a class.\n",
    "• This code snippet is used to convert the predicted probabilities of a classification model \n",
    "  into predicted classes that can be used for further analysis or evaluation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is written in Python.\n",
    "• The code snippet is simply printing the shapes of two arrays, predicted_classes and test_Y.\n",
    "• The shape attribute of a NumPy array returns a tuple representing the dimensions of the array.\n",
    "• This code is printing the dimensions of the two arrays to help the user understand the size and \n",
    "  shape of the data they are working with.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_classes.shape, test_Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code creates a tuple with two elements, each of which is a tuple with one integer element.\n",
    "• The integers in both tuples are the same, 10000.\n",
    "• The comma after each integer is necessary to indicate that the tuple has only one element.\n",
    "• This code is written in Python.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "((10000,), (10000,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is used to visualize the correctly predicted labels in a classification problem.\n",
    "• First, the np.where() function is used to find the indices where the predicted classes match the \n",
    "  actual test labels (test_Y).\n",
    "• The resulting array of indices is stored in the correct variable.\n",
    "• Next, the number of correct labels is printed using the len() function and string formatting.\n",
    "• Then, a for loop is used to iterate over the first 9 correct predictions.\n",
    "• Within the loop, plt.subplot() is used to create a grid of subplots with 3 rows and 3 columns.\n",
    "• The plt.imshow() function is used to display the corresponding image from the test set (test_X) \n",
    "  at the current index (correct).\n",
    "• The cmap parameter is set to 'gray' to display the image in grayscale, and interpolation is set to \n",
    "  'none' to display the image without any interpolation.\n",
    "• The plt.title() function is used to display the predicted class and the actual class for the current image.\n",
    "• The plt.tight_layout() is used to adjust the spacing between the subplots.\n",
    "• This code snippet is a useful tool for visualizing the performance of a classification model \n",
    "  by displaying the correctly predicted labels.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct = np.where(predicted_classes==test_Y)[0]\n",
    "print \"Found %d correct labels\" % len(correct)\n",
    "for i, correct in enumerate(correct[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], test_Y[correct]))\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c80db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is printing out a message that says \"Found 9188 correct labels\".\n",
    "• It does not involve any programming language as it is just a plain text output.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Found 9188 correct labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code is used to visualize the incorrectly predicted labels in a classification problem.\n",
    "• First, the code uses the NumPy where function to find the indices of the incorrectly predicted labels \n",
    "  by comparing the predicted classes with the actual test labels.\n",
    "• These indices are stored in the incorrect variable.\n",
    "• Next, the code prints the number of incorrect labels found using the len function and a formatted string.\n",
    "• Then, a for loop is used to iterate over the first 9 incorrect labels.\n",
    "• Within the loop, the subplot function from Matplotlib is used to create a grid of 3x3 subplots.\n",
    "• The imshow function is used to display the image corresponding to the current incorrect label, \n",
    "  which is accessed using the test_X array and the current index from the incorrect variable.\n",
    "• The title function is used to display the predicted class and the actual class for the current image.\n",
    "• Finally, the tight_layout function is used to adjust the spacing between the subplots.\n",
    "• This code provides a visual representation of the incorrectly predicted labels, which can help \n",
    "  in understanding the performance of a classification model.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "incorrect = np.where(predicted_classes!=test_Y)[0]\n",
    "print \"Found %d incorrect labels\" % len(incorrect)\n",
    "for i, incorrect in enumerate(incorrect[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], test_Y[incorrect]))\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2114ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet is a print statement that outputs the message \"Found 812 incorrect labels\".\n",
    "• It does not require any specific programming language as it is just a plain text output.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f83f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Found 812 incorrect labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code imports the classification_report function from the sklearn.metrics module.\n",
    "• It then creates a list of target names using a list comprehension that formats the string \n",
    "  \"Class {}\" with the range of num_classes.\n",
    "• It prints the classification report by calling the classification_report function with the test_Y and \n",
    "  predicted_classes arrays as arguments, and passing the target_names list as the target_names parameter.\n",
    "• The classification report provides a summary of the precision, recall, and F1 score for each class \n",
    "  in the target_names list, as well as the overall accuracy of the model.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(test_Y, predicted_classes, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ce2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    ". This code snippet shows the classification report of a machine learning model's performance on a dataset.\n",
    "• The report includes precision, recall, and f1-score for each class in the dataset, as well as the support \n",
    "  (number of samples) for each class.\n",
    "• The \"avg/total\" row shows the weighted average of precision, recall, and f1-score across all classes.\n",
    "• Precision is the ratio of true positives to the total number of predicted positives, \n",
    "  while recall is the ratio of true positives to the total number of actual positives.\n",
    "• F1-score is the harmonic mean of precision and recall.\n",
    "• This report can be generated using the classification_report function from the sklearn.metrics module in Python.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ade73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "    Class 0       0.77      0.90      0.83      1000\n",
    "    Class 1       0.99      0.98      0.99      1000\n",
    "    Class 2       0.88      0.88      0.88      1000\n",
    "    Class 3       0.94      0.92      0.93      1000\n",
    "    Class 4       0.88      0.87      0.88      1000\n",
    "    Class 5       0.99      0.98      0.98      1000\n",
    "    Class 6       0.82      0.72      0.77      1000\n",
    "    Class 7       0.94      0.99      0.97      1000\n",
    "    Class 8       0.99      0.98      0.99      1000\n",
    "    Class 9       0.98      0.96      0.97      1000\n",
    "\n",
    "avg / total       0.92      0.92      0.92     10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The classifier is underperforming for class 6 regarding both precision and recall. \n",
    "For class 0 and class 2, the classifier is lacking precision. Also, for class 4, the classifier \n",
    "is slightly lacking both precision and recall.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
